{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a1bd40fb",
      "metadata": {
        "id": "a1bd40fb"
      },
      "source": [
        "# Batched Wikidata Scraper for News Media\n",
        "\n",
        "This notebook retrieves detailed information about news and media organisations from **Wikidata**, including their social media profiles (Twitter, Facebook, Instagram, TikTok, YouTube) and organisational metadata such as headquarters, country, language, inception date, owner, and parent organisation.\n",
        "\n",
        "To respect the limits of the SPARQL endpoint and avoid timeouts, the scraper processes the Q-IDs in batches and pauses between requests as configured.  Results are saved both in raw and deduplicated form as CSV and JSON files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e142b22",
      "metadata": {
        "id": "6e142b22"
      },
      "source": [
        "## Optional: Mount Google Drive\n",
        "\n",
        "If you are running this notebook on Google Colab and wish to save the output files to your Drive, run the following cell.  On other platforms, you can omit this step or modify `OUTPUT_DIR` in the configuration section below to a local path.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d10bcc3",
      "metadata": {
        "id": "4d10bcc3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Mount Google Drive (only required on Google Colab)\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18ab7364",
      "metadata": {
        "id": "18ab7364"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "Set the output directory and batching parameters here.  The `BATCH_SIZE` determines how many Q-IDs are requested per SPARQL query; `SLEEP_SEC` controls the pause between batches to avoid overloading the endpoint.  When running on Colab, ensure that `OUTPUT_DIR` points to a directory you have write permissions for (for example, a folder within your Drive).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67893855",
      "metadata": {
        "id": "67893855"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "# Directory where CSV/JSON files will be saved.  Change this as needed.\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/YouTube/SPARQL/\"\n",
        "\n",
        "# Number of Q-IDs to request per SPARQL query.  A smaller batch size may reduce the chance of timeouts.\n",
        "BATCH_SIZE = 25\n",
        "\n",
        "# Seconds to sleep between batch requests.  Increase this if you encounter rate limiting or need to throttle requests.\n",
        "SLEEP_SEC = 0.0\n",
        "\n",
        "# Create the output directory if it does not already exist\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"Files will be written to: {OUTPUT_DIR}\")\n",
        "print(f\"BATCH_SIZE = {BATCH_SIZE}   |   SLEEP = {SLEEP_SEC} s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ca12990",
      "metadata": {
        "id": "2ca12990"
      },
      "source": [
        "## Helper functions\n",
        "\n",
        "The following functions assist with making requests to the SPARQL endpoint, safely extracting values from `pandas` rows, and building the `VALUES` block of SPARQL queries for batched requests.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e13b880",
      "metadata": {
        "id": "2e13b880"
      },
      "outputs": [],
      "source": [
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import math\n",
        "import textwrap\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# SPARQL endpoint and headers for Wikidata queries\n",
        "ENDPOINT = \"https://query.wikidata.org/sparql\"\n",
        "HEADERS = {\"User-Agent\": \"NewsYTMapping/3.0 (Colab; contact: your_email@example.com)\"}\n",
        "\n",
        "\n",
        "def run_query(query: str) -> pd.DataFrame:\n",
        "    \"\"\"Execute a SPARQL query and return the results as a DataFrame.\n",
        "\n",
        "    Raises an HTTP error if the request fails.\n",
        "    \"\"\"\n",
        "    response = requests.get(\n",
        "        ENDPOINT,\n",
        "        params={\"query\": query, \"format\": \"json\"},\n",
        "        headers=HEADERS,\n",
        "        timeout=60,\n",
        "    )\n",
        "    response.raise_for_status()\n",
        "    return pd.DataFrame([\n",
        "        {k: v[\"value\"] for k, v in b.items()}\n",
        "        for b in response.json()[\"results\"][\"bindings\"]\n",
        "    ])\n",
        "\n",
        "\n",
        "def safe_get(series: pd.Series, key: str) -> str:\n",
        "    \"\"\"Safely extract a value from a pandas Series, returning an empty string if missing or NaN.\"\"\"\n",
        "    val = series.get(key, \"\")\n",
        "    return val if (pd.notna(val) and val != \"\") else \"\"\n",
        "\n",
        "\n",
        "def build_values_block(qids: list) -> str:\n",
        "    \"\"\"Build a SPARQL VALUES clause from a list of Q-IDs.\"\"\"\n",
        "    inside = \" \".join(f\"wd:{qid}\" for qid in qids)\n",
        "    return f\"VALUES ?item {{ {inside} }}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49951765",
      "metadata": {
        "id": "49951765"
      },
      "source": [
        "## Collect news-media Q-IDs\n",
        "\n",
        "This step retrieves the Wikidata identifiers (Q-IDs) for all items that are classified as news/media organisations (instances or subclasses of `Q1193236`) and have an official website (`P856`).  The Q-IDs are fetched in chunks to avoid timeouts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d27a7255",
      "metadata": {
        "id": "d27a7255"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Collect Q-IDs for news/media organisations with an official website\n",
        "items = []\n",
        "chunk_size = 10000\n",
        "offset = 0\n",
        "print(\"Gathering Q-IDs (news media) with an official website …\")\n",
        "while True:\n",
        "    query_chunk = f\"\"\"\n",
        "    SELECT ?item WHERE {{\n",
        "      ?item wdt:P856 [] .\n",
        "      ?item wdt:P31/wdt:P279* wd:Q1193236 .\n",
        "    }} LIMIT {chunk_size} OFFSET {offset}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df_chunk = run_query(query_chunk)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  Chunk offset {offset} failed: {e}\")\n",
        "        offset += chunk_size\n",
        "        time.sleep(SLEEP_SEC)\n",
        "        continue\n",
        "    if df_chunk.empty:\n",
        "        break\n",
        "    # Extract Q-IDs from the full URLs\n",
        "    items.extend(df_chunk[\"item\"].str.split(\"/\").str[-1])\n",
        "    offset += chunk_size\n",
        "    print(f\"  – collected {len(items):,}\")\n",
        "    time.sleep(SLEEP_SEC)\n",
        "\n",
        "print(f\"✓ Total news-media Q-IDs: {len(items):,}\n",
        "\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44e3e879",
      "metadata": {
        "id": "44e3e879"
      },
      "source": [
        "## Batched retrieval of organisation data\n",
        "\n",
        "With the list of Q-IDs in hand, the notebook builds a SPARQL query for each batch.  Each query requests a variety of social media properties and organisation metadata, using `OPTIONAL` clauses so that missing data does not exclude an organisation from the results.  Between batches, the script pauses for `SLEEP_SEC` seconds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e69afacf",
      "metadata": {
        "id": "e69afacf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Retrieve organisation data in batches\n",
        "basic_rows = []\n",
        "full_rows = []\n",
        "batches = math.ceil(len(items) / BATCH_SIZE)\n",
        "\n",
        "for batch_index in range(batches):\n",
        "    batch_qids = items[batch_index * BATCH_SIZE : (batch_index + 1) * BATCH_SIZE]\n",
        "    values_clause = build_values_block(batch_qids)\n",
        "    sparql_batch = textwrap.dedent(f\"\"\"\n",
        "    SELECT ?item ?itemLabel ?official_website ?twitter ?facebook ?instagram ?tiktok\n",
        "           ?youtube_channel ?youtube_handle ?youtube_playlists ?headquartersLabel\n",
        "           ?countryLabel ?languageLabel ?inception ?ownerLabel ?parentOrgLabel ?subscribers\n",
        "    WHERE {{\n",
        "      {values_clause}\n",
        "      ?item wdt:P856 ?official_website .\n",
        "      OPTIONAL {{ ?item wdt:P2002 ?twitter. }}\n",
        "      OPTIONAL {{ ?item wdt:P2013 ?facebook. }}\n",
        "      OPTIONAL {{ ?item wdt:P2003 ?instagram. }}\n",
        "      OPTIONAL {{ ?item wdt:P7085 ?tiktok. }}\n",
        "      OPTIONAL {{ ?item wdt:P2397 ?youtube_channel. }}\n",
        "      OPTIONAL {{ ?item wdt:P11245 ?youtube_handle. }}\n",
        "      OPTIONAL {{ ?item wdt:P4300 ?youtube_playlists. }}\n",
        "      OPTIONAL {{ ?item wdt:P159 ?headquarters. }}\n",
        "      OPTIONAL {{ ?item wdt:P17 ?country. }}\n",
        "      OPTIONAL {{ ?item wdt:P407 ?language. }}\n",
        "      OPTIONAL {{ ?item wdt:P571 ?inception. }}\n",
        "      OPTIONAL {{ ?item wdt:P127 ?owner. }}\n",
        "      OPTIONAL {{ ?item wdt:P749 ?parentOrg. }}\n",
        "      OPTIONAL {{\n",
        "        ?item p:P2397 ?ytStmt .\n",
        "        ?ytStmt ps:P2397 ?youtube_channel .\n",
        "        ?ytStmt pq:P3744 ?subscribers .\n",
        "      }}\n",
        "      SERVICE wikibase:label {{\n",
        "        bd:serviceParam wikibase:language \"en\".\n",
        "        ?item rdfs:label ?itemLabel .\n",
        "        ?headquarters rdfs:label ?headquartersLabel .\n",
        "        ?country rdfs:label ?countryLabel .\n",
        "        ?language rdfs:label ?languageLabel .\n",
        "        ?owner rdfs:label ?ownerLabel .\n",
        "        ?parentOrg rdfs:label ?parentOrgLabel .\n",
        "      }}\n",
        "    }}\n",
        "    \"\"\")\n",
        "    try:\n",
        "        df = run_query(sparql_batch)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  Batch {batch_index+1}/{batches} failed: {e} – skipped.\")\n",
        "        time.sleep(SLEEP_SEC)\n",
        "        continue\n",
        "\n",
        "    if df.empty:\n",
        "        time.sleep(SLEEP_SEC)\n",
        "        continue\n",
        "\n",
        "    # Collect full records\n",
        "    full_rows.extend(df.to_dict(\"records\"))\n",
        "\n",
        "    # Collect basic records (one per item) for a simpler mapping\n",
        "    for qid in batch_qids:\n",
        "        sub = df[df[\"item\"].str.endswith(qid)]\n",
        "        if sub.empty:\n",
        "            continue\n",
        "        r0 = sub.iloc[0]\n",
        "        youtube_id = safe_get(r0, \"youtube_channel\")\n",
        "        yt_url = f\"https://www.youtube.com/channel/{youtube_id}\" if youtube_id else \"\"\n",
        "        basic_rows.append({\n",
        "            \"item\": qid,\n",
        "            \"name\": safe_get(r0, \"itemLabel\"),\n",
        "            \"news_domain\": safe_get(r0, \"official_website\"),\n",
        "            \"youtube_url\": yt_url,\n",
        "        })\n",
        "\n",
        "    # Progress logging\n",
        "    if (batch_index + 1) % 10 == 0 or batch_index + 1 == batches:\n",
        "        done = min((batch_index + 1) * BATCH_SIZE, len(items))\n",
        "        print(f\"✓ batches {batch_index+1}/{batches}  |  items {done:,}/{len(items):,}\")\n",
        "    time.sleep(SLEEP_SEC)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6686ab7a",
      "metadata": {
        "id": "6686ab7a"
      },
      "source": [
        "## Deduplicate and save results\n",
        "\n",
        "After retrieving data for all batches, we create DataFrames from the collected records, deduplicate them by `item` (Q-ID), and write four files to the output directory:\n",
        "\n",
        "- **news_domain_youtube_raw** – one row per item per record (may include duplicates);\n",
        "- **news_domain_youtube_unique** – unique items with basic data (one row per organisation);\n",
        "- **all_info_raw** – raw full data with all retrieved fields;\n",
        "- **all_info_unique** – unique full data (one row per organisation).\n",
        "\n",
        "Each file is saved in both CSV and JSON formats and can be uploaded to GitHub alongside this notebook for further analysis or reuse.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab4cbb0b",
      "metadata": {
        "id": "ab4cbb0b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convert the collected data to DataFrames\n",
        "df_basic_raw = pd.DataFrame(basic_rows)\n",
        "df_full_raw  = pd.DataFrame(full_rows)\n",
        "\n",
        "# Deduplicate by Q-ID\n",
        "df_basic_unique = df_basic_raw.drop_duplicates(subset=[\"item\"])\n",
        "df_full_unique  = df_full_raw.drop_duplicates(subset=[\"item\"])\n",
        "\n",
        "# Function to save a DataFrame to CSV and JSON\n",
        "def save_df(df: pd.DataFrame, stem: str):\n",
        "    csv_path  = os.path.join(OUTPUT_DIR, f\"{stem}.csv\")\n",
        "    json_path = os.path.join(OUTPUT_DIR, f\"{stem}.json\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    df.to_json(json_path, orient=\"records\", force_ascii=False, indent=2)\n",
        "    print(f\"  • {stem}  ({len(df):,} rows)\")\n",
        "\n",
        "print(\"\n",
        "Writing files …\")\n",
        "save_df(df_basic_raw , \"news_domain_youtube_raw\")\n",
        "save_df(df_basic_unique , \"news_domain_youtube_unique\")\n",
        "save_df(df_full_raw  , \"all_info_raw\")\n",
        "save_df(df_full_unique , \"all_info_unique\")\n",
        "\n",
        "print(\"\n",
        "✓ Finished. Files saved to the output directory above.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3f622f1",
      "metadata": {
        "id": "c3f622f1"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates how to perform a batched retrieval of news/media organisations from Wikidata, capturing a wide range of social media and organisational metadata.  By batching requests, respecting the SPARQL endpoint’s limits and adding appropriate pauses, it is possible to collect detailed datasets for thousands of entities.\n",
        "\n",
        "Feel free to modify the SPARQL query to include other properties or to tailor the dataset to your needs.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}