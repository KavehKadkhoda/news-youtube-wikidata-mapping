{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ead38699",
      "metadata": {
        "id": "ead38699"
      },
      "source": [
        "# News Domain YouTube Mapping\n",
        "\n",
        "This notebook queries **Wikidata** for information about news and media organisations and their YouTube channels.\n",
        "\n",
        "The goal is to produce a cleaned and deduplicated dataset that maps each organisation to its YouTube channel information (channel ID, handle, title) and website domain.  The final dataset is exported to CSV and JSON formats for easy reuse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7c61ace2",
      "metadata": {
        "id": "7c61ace2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import standard libraries for HTTP requests, CSV/JSON handling and URL parsing\n",
        "import requests\n",
        "import csv\n",
        "import json\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# In a Google Colab environment, `google.colab.files` provides helpers for downloading files.\n",
        "# If you are not using Colab, you can remove this import and the calls to `files.download()`.\n",
        "from google.colab import files\n",
        "\n",
        "# Define a SPARQL query to retrieve news organisations and their YouTube data\n",
        "SPARQL = \"\"\"\n",
        "SELECT DISTINCT ?item ?itemLabel ?website ?ytid ?yhandle ?channelTitle WHERE {\n",
        "    # 1. News/media organisations (instance-of subtree of news organisation)\n",
        "    ?item wdt:P31/wdt:P279* wd:Q1193236 ;\n",
        "          wdt:P856 ?website ;\n",
        "          wdt:P2397 ?ytid .\n",
        "    # 2. Optional YouTube handle (main value)\n",
        "    OPTIONAL { ?item wdt:P11245 ?yhandle . }\n",
        "    # 3. Optional channel title (qualifier on the P2397 statement)\n",
        "    OPTIONAL {\n",
        "        ?item p:P2397 ?ytStmt .\n",
        "        ?ytStmt ps:P2397 ?ytid .\n",
        "        OPTIONAL { ?ytStmt pq:P1932 ?channelTitle . }\n",
        "    }\n",
        "    # 4. Get the organisation's label in the UI language\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
        "}\n",
        "\"\"\"\n",
        "# Set the endpoint for Wikidata and HTTP request headers (identifying our script)\n",
        "ENDPOINT = \"https://query.wikidata.org/sparql\"\n",
        "HEADERS = {\n",
        "    \"Accept\": \"application/sparql-results+json\",\n",
        "    # Identify yourself with a descriptive user-agent; replace the email with your contact\n",
        "    \"User-Agent\": \"NewsYTMapping/1.2 (Google Colab; contact: your_email@example.com)\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93f70e74",
      "metadata": {
        "id": "93f70e74"
      },
      "source": [
        "\n",
        "## Understanding the SPARQL query\n",
        "\n",
        "The SPARQL query above retrieves:\n",
        "\n",
        "- Each news or media organisation item and its label (using the `item` and `itemLabel` variables).\n",
        "- The official website (`P856`) associated with each organisation.  We later use the domain of this URL for deduplication.\n",
        "- The YouTube channel identifier (`P2397`) for the organisation (stored in `ytid`).\n",
        "- An optional YouTube handle (`P11245`), if recorded (`yhandle`).\n",
        "- An optional YouTube channel title qualifier (`P1932` on the statement of `P2397`), stored in `channelTitle`.\n",
        "\n",
        "It also uses the `wikibase:label` service to return labels in the user's interface language (falling back to English when necessary).\n",
        "\n",
        "By specifying `SELECT DISTINCT` we avoid duplicate rows at the SPARQL level, although further deduplication is needed because multiple rows can still refer to the same organisation and channel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c679425b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c679425b",
        "outputId": "d04e7a62-d469-45fc-ac0a-3c4bad0a9915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved 2264 unique outlets from 2457 raw rows\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Query Wikidata for the rows defined by the SPARQL query\n",
        "response = requests.get(ENDPOINT, params={'query': SPARQL}, headers=HEADERS, timeout=60)\n",
        "response.raise_for_status()  # Raise an exception if the request failed\n",
        "rows = response.json()[\"results\"][\"bindings\"]\n",
        "\n",
        "# Transform and deduplicate results\n",
        "# We'll use a dictionary keyed by (domain, channel ID) to ensure each organisation-channel pair only appears once.\n",
        "unique = {}\n",
        "for row in rows:\n",
        "    # Extract the website URL and parse out the domain (remove the common 'www.' prefix)\n",
        "    site = row[\"website\"][\"value\"]\n",
        "    domain = urlparse(site).split(\"/\")[0] if hasattr(urlparse, 'split') else urlparse(site).netloc.lower().replace(\"www.\", \"\")\n",
        "    # Note: We'll handle domain extraction more robustly using urlparse below\n",
        "    from urllib.parse import urlparse as _urlparse\n",
        "    domain = _urlparse(site).netloc.lower().replace(\"www.\", \"\")\n",
        "\n",
        "    # Extract YouTube channel ID\n",
        "    ytid = row[\"ytid\"][\"value\"]\n",
        "    key = (domain, ytid)\n",
        "\n",
        "    # Extract optional YouTube handle and channel title if available\n",
        "    handle = row.get(\"yhandle\", {}).get(\"value\", \"\")\n",
        "    title  = row.get(\"channelTitle\", {}).get(\"value\", \"\")\n",
        "\n",
        "    if key not in unique:\n",
        "        # Create a new entry when encountering a domain-channel combination for the first time\n",
        "        unique[key] = {\n",
        "            \"news_name\": row[\"itemLabel\"][\"value\"],\n",
        "            \"domain\": domain,\n",
        "            \"youtube_channel_id\": ytid,\n",
        "            \"youtube_url\": f\"https://www.youtube.com/channel/{ytid}\",\n",
        "            \"youtube_handle\": handle,\n",
        "            \"youtube_channel_title\": title,\n",
        "        }\n",
        "    else:\n",
        "        # If the key already exists, fill in any missing fields from additional rows\n",
        "        record = unique[key]\n",
        "        if not record[\"youtube_handle\"] and handle:\n",
        "            record[\"youtube_handle\"] = handle\n",
        "        if not record[\"youtube_channel_title\"] and title:\n",
        "            record[\"youtube_channel_title\"] = title\n",
        "\n",
        "# Convert the dictionary of unique records to a list for saving\n",
        "data = list(unique.values())\n",
        "print(f\"Retrieved {len(data)} unique outlets from {len(rows)} raw rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2d8081c",
      "metadata": {
        "id": "b2d8081c"
      },
      "source": [
        "\n",
        "## Deduplication strategy\n",
        "\n",
        "After retrieving all rows from Wikidata, there may still be duplicates because a single news organisation can appear multiple times if it has qualifiers or multiple statements.\n",
        "\n",
        "The deduplication step groups rows by the combination of the organisation's website domain and YouTube channel ID.  For each unique combination, it keeps the first occurrence and then fills in missing values for the YouTube handle or channel title if subsequent rows provide them.\n",
        "\n",
        "This yields a clean dataset in which each news outlet appears only once.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8d923c05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8d923c05",
        "outputId": "2a8aa5a8-4f64-427e-a77f-9a26c7730e39"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e7049f31-a862-4537-b237-1daf5b6d59cd\", \"news_youtube_mapping.csv\", 263219)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d9d17176-050e-4298-91dd-6f493ee6b6fb\", \"news_youtube_mapping.json\", 607259)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "# Save the cleaned data to CSV and JSON formats\n",
        "csv_filename = \"news_youtube_mapping.csv\"\n",
        "json_filename = \"news_youtube_mapping.json\"\n",
        "\n",
        "# Write the CSV file\n",
        "with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=data[0].keys())\n",
        "    writer.writeheader()\n",
        "    writer.writerows(data)\n",
        "\n",
        "# Write the JSON file\n",
        "with open(json_filename, \"w\", encoding=\"utf-8\") as jsonfile:\n",
        "    json.dump(data, jsonfile, indent=2, ensure_ascii=False)\n",
        "\n",
        "# If running in Google Colab, make the files available for download\n",
        "files.download(csv_filename)\n",
        "files.download(json_filename)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8713703c",
      "metadata": {
        "id": "8713703c"
      },
      "source": [
        "\n",
        "## Summary and next steps\n",
        "\n",
        "This notebook demonstrates how to use the Wikidata SPARQL endpoint to retrieve a curated list of news organisations and their YouTube channels.  The data is deduplicated by domain and channel ID, ensuring that each outlet appears only once.  You can explore or visualize the resulting CSV/JSON files, or combine them with other datasets for further analysis.\n",
        "\n",
        "Feel free to adapt the SPARQL query to include additional fields (for example, country of headquarters or organisation type) or to filter for specific types of media outlets.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}